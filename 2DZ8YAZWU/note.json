{
  "paragraphs": [
    {
      "text": "%md \n\n### Spark ML 机器学习LAB- 森林植被预测",
      "user": "hadoop",
      "dateUpdated": "2019-01-03 22:28:58.807",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eSpark ML 机器学习LAB- 森林植被预测\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1546525240755_945428881",
      "id": "20181229-154554_1028883481",
      "dateCreated": "2019-01-03 22:20:40.755",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n- 初始化环境",
      "user": "hadoop",
      "dateUpdated": "2019-01-03 22:20:40.762",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n  \u003cli\u003e初始化环境\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1546525240761_1009873402",
      "id": "20181229-155141_853775782",
      "dateCreated": "2019-01-03 22:20:40.761",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import scala.util.Random\n\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.DataFrame\n\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.ml.feature.{VectorIndexer, VectorAssembler}\nimport org.apache.spark.ml.classification.{DecisionTreeClassifier,\n                                           RandomForestClassifier,\n                                           RandomForestClassificationModel}\n\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
      "user": "hadoop",
      "dateUpdated": "2019-01-03 22:21:12.991",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import scala.util.Random\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.ml.feature.{VectorIndexer, VectorAssembler}\nimport org.apache.spark.ml.classification.{DecisionTreeClassifier, RandomForestClassifier, RandomForestClassificationModel}\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1546525240762_741197540",
      "id": "20181229-155155_182595759",
      "dateCreated": "2019-01-03 22:20:40.762",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// hdfs 路径\nval dir_base \u003d \"hdfs:///user/hadoop/spark/covtype\"\n\nval dataWithoutHeaded \u003d spark.read.option(\"inferSchema\", true).option(\"header\", false).csv(s\"${dir_base}/covtype.data\")\n  ",
      "user": "hadoop",
      "dateUpdated": "2019-01-03 22:20:40.763",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "dir_base: String \u003d hdfs:///user/hadoop/spark/covtype\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 4 times, most recent failure: Lost task 0.3 in stage 15.0 (TID 40, localhost, executor 1): java.io.InvalidClassException: org.apache.commons.lang3.time.FastDateParser; local class incompatible: stream classdesc serialVersionUID \u003d 2, local class serialVersionUID \u003d 3\n\tat java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n  at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1124)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1117)\n  at org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$.infer(CSVInferSchema.scala:44)\n  at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.inferFromDataset(CSVDataSource.scala:257)\n  at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:233)\n  at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:68)\n  at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n  at scala.Option.orElse(Option.scala:289)\n  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:617)\n  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:468)\n  ... 56 elided\nCaused by: java.io.InvalidClassException: org.apache.commons.lang3.time.FastDateParser; local class incompatible: stream classdesc serialVersionUID \u003d 2, local class serialVersionUID \u003d 3\n  at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:699)\n  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1885)\n  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)\n  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)\n  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)\n  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)\n  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n  at org.apache.spark.scheduler.Task.run(Task.scala:121)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n  ... 3 more\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1546525240763_-1666781403",
      "id": "20181229-155318_361099773",
      "dateCreated": "2019-01-03 22:20:40.763",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "hadoop",
      "dateUpdated": "2019-01-03 22:20:40.764",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1546525240764_965880380",
      "id": "20181229-155244_1250204333",
      "dateCreated": "2019-01-03 22:20:40.764",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "spark/ml",
  "id": "2DZ8YAZWU",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}